{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP: 19999 Score: 200.0         \r"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "NUM_EP=20000\n",
    "LR=0.000025\n",
    "GAMMA=0.99\n",
    "\n",
    "env=gym.make('CartPole-v0')\n",
    "actionspace=env.action_space.n\n",
    "np.random.seed(1)\n",
    "\n",
    "w=np.random.rand(4,2)\n",
    "\n",
    "def policy(state,w):\n",
    "    z=state.dot(w)\n",
    "    exp=np.exp(z)\n",
    "    return exp/np.sum(exp)\n",
    "\n",
    "def grad_soft(softmax):\n",
    "    s=softmax.reshape(-1,1)\n",
    "    return np.diagflat(s)-np.dot(s,s.T)\n",
    "\n",
    "def select_action(actionspace,probs):\n",
    "    return np.random.choice(actionspace,p=probs[0])\n",
    "\n",
    "def update(w,grads,rewards,LR,GAMMA):\n",
    "    for e in range(len(grads)):\n",
    "        w+=LR*grads[e]*sum([r*(GAMMA**r)for t,r in enumerate(rewards[e:])])\n",
    "    return w \n",
    "\n",
    "\n",
    "def next_episde(w,state,actionspace):\n",
    "    probs=policy(state,w)\n",
    "    action=select_action(actionspace,probs)\n",
    "    statee,reward, done,_ = env.step(action)\n",
    "    statee=statee[None,:]\n",
    "            \n",
    "            \n",
    "    dsoft=grad_soft(probs)[action,:]\n",
    "    log=dsoft/ probs[0,action]\n",
    "    delo=state.T.dot(log[None,:])\n",
    "    return delo,reward,statee,done\n",
    "\n",
    "def train(w,actionspace,NUM_EP,GAMMA,LR):\n",
    "   \n",
    "    for i in range(NUM_EP):\n",
    "        grads=[]\n",
    "        rewards=[]\n",
    "        ep_rewards=[]\n",
    "        score=0\n",
    "        state=env.reset()[None,:]\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            delo, reward,statee, done=next_episde(w,state,actionspace)\n",
    "            grads.append(delo)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "            score+=reward\n",
    "            state=statee\n",
    "            if done:\n",
    "                break\n",
    "                #env.reset()\n",
    "        w=update(w,grads,rewards,LR,GAMMA)\n",
    "        ep_rewards.append(score)\n",
    "        print(\"EP: \" + str(i)+ \" Score: \" +\"Weight:\" +str(w)+str(score) + \"         \",end=\"\\r\", flush=False) \n",
    "        #rint(w)\n",
    "        env.close()\n",
    "    \n",
    "state = env.reset()[None,:]\n",
    "train(w,actionspace,NUM_EP,GAMMA,LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning introduction: Vanilla Policy Gradient \n",
    "![alt text](reinfo.jpg)\n",
    "The basic idea behind deep reinforcement learning is that an agent in an envionment with a given state has a policy, which is typically a convolutional neural net that predicts the probability of the next action, then the as agent observes the next state the parameters of the policy can be slowly learned. The agent learns which actions are \"good\"  by the rewards recieved by taking a particular action in a certain state.  Mathematically the trajectory of the agent is given by the sequence of the state and action over an \"episode\". The goal of policy gradient is to train our policy parameters by running Monte Carlo simulations over many episodes in order to maximize to discounted expected reward. The vanilla policy gradient sets the stage for classes of more advanced RL algorthims such as actor critic and Q-learning methods. \n",
    "\\begin{array}{rcl}\n",
    "Max_\\theta E_\\pi R(T)\n",
    "\\end{array}\n",
    "The parameters of the policy are learned via gradient descent \n",
    "\\begin{array}{rcl}\n",
    "\\theta_{t+1} &=& \\theta_t +\\alpha \\nabla_\\theta E_\\pi R(T)\n",
    "\\end{array}\n",
    "by exploiting properties of logarithm and simple algebraic manipulation the gradient of the expected reward is given by:\n",
    "\\begin{array}{rcl}    \n",
    "\\nabla_\\theta E_\\pi R(T) &=& E_\\pi \\nabla_\\theta log (P(t | \\theta) R(T))\n",
    "\\end{array}\n",
    "in a Makrov decision process the conditional probability over a trajectory is given by \n",
    "\\begin{array}\n",
    "(P(t | \\theta)=p(s_0) \\prod_0^{T-1} p(s_{t+1}|s_t,a_t) \\pi_{\\theta}(a_t,s_t)\n",
    "\\end{array}\n",
    "This yields the equation for the gradient of the expected reward for Monte Carlo sampling:\n",
    "\\begin{array}\n",
    "\\\\ \\nabla_\\theta E_\\pi R(T) &=& \\frac{1}{M} \\Sigma_T \\Sigma_{t=0}^{T-1} log  \\pi_\\theta R(T)\n",
    "\\end{array}\n",
    "This was not a formal derivation, all the details of the derivation can be found in the openAI docs. Instead of using a convolutional neural net we will keep it simple and use a simple logistic regression model as our Pi_theta.\n",
    "\n",
    "Tutorial: https://www.janisklaise.com/post/rl-policy-gradients/\n",
    "\n",
    "Another great tutorial: http://quant.am/cs/2017/08/07/policy-gradients/\n",
    "\n",
    "ANother tutorial: https://medium.com/samkirkiles/reinforce-policy-gradients-from-scratch-in-numpy-6a09ae0dfe12\n",
    "\n",
    "derivation: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient\n",
    "\n",
    "Likely the best notes on the subject matter, CS 285 from Berkley( would recommend the whole course); there are notes from one of researchers who discovered the PPO algorthim in 2016 Sergey Levine: https://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf\n",
    "\n",
    "The main issue with the policy gradient is that it has a rather high varinace and hence has a slow convergence rate taking often many iterations. It is also difficult to select hyperparameters in the optimization process such as the learning rate, for particular optimization algorthims it is certainly easier, for example in the Berkley notes Levin mentions that Adam's step size is not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py:151: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  \"You are calling 'step()' even though this \"\n"
     ]
    }
   ],
   "source": [
    "# note this is the W i obtained from only 5000 iterations it requires 20,000 to fully learn\n",
    "while True:\n",
    "    w=[[ 1.06721556,0.07013094],    \n",
    " [-0.71238789,1.01483483],\n",
    " [-1.22409825,1.46319274],\n",
    " [-2.51589573,3.04771667]]\n",
    "    #env.render()\n",
    "    probs=policy(state,w)\n",
    "    action=select_action(actionspace,probs)\n",
    "    statee,reward, done,_ = env.step(action)\n",
    "    statee=statee[None,:]\n",
    "    env.render()\n",
    "    env.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
