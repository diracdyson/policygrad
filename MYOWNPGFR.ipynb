{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats \n",
    "import gym\n",
    "import random\n",
    "# create a class for our policy gradient\n",
    "class PolicyG(object):\n",
    "    # initalize our variables with constructor\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.theta=np.random.random(4)\n",
    "        self.alpha=0.01\n",
    "        self.gamma=0.99\n",
    "        self.actions=[]\n",
    "        self.probs=[]\n",
    "        self.states=[]\n",
    "        self.rewards=[]\n",
    "        \n",
    "    # inconjunction with the variabler names outputed by Gym env\n",
    "    ## append our data each loop\n",
    "    def memory(self,state,action,prob,reward):\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(prob)\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return (1)/(1+np.exp(-x))\n",
    "    \n",
    "    def prob(self,x):\n",
    "        # prob_0 is the model pi_0 like a logisitc regression model where theta is the parameter\n",
    "        # and x is the data input in this action it is action array from the gym virtual env\n",
    "        #trivially it is p, and 1-p by laws of prob\n",
    "        y=x@self.theta\n",
    "        prob0=self.sigmoid(y)\n",
    "        return np.array([prob0,1-prob0])\n",
    "    # sample action from our dist\n",
    "    \n",
    "    def act(self,x):\n",
    "        # we sample from our logisitc regression function\n",
    "        prob1=self.prob(x)\n",
    "        action=np.random.choice([0,1],p=prob1)\n",
    "        return action, prob1[action]\n",
    "    \n",
    "    def grad_log(self,x):\n",
    "        #print(self.actions)\n",
    "        print(self.theta)\n",
    "        #print(x[0])\n",
    "        y=x@self.theta \n",
    "        grad_p0=np.array(x)-np.array(x)*self.sigmoid(y)\n",
    "        grad_p1=-np.array(x)*self.sigmoid(y)\n",
    "        \n",
    "        return grad_p0,grad_p1\n",
    "    \n",
    "    \n",
    "    # gradient dotted with reward of action in a single episode \n",
    "    #def grad_log_dot_reward(self,grad_log,discounted_rewards):\n",
    "      #  return grad_log.T@discounted_rewards\n",
    "    \n",
    "    def discount_rewards(self):\n",
    "        \n",
    "        discounted_rewards=np.zeros(len(self.rewards))\n",
    "        \n",
    "        cum_rewards=0\n",
    "        for i in reversed(range(0,len(self.rewards))):\n",
    "            ## past events matter less by gamma param\n",
    "            cum_rewards= cum_rewards +self.rewards[i]+self.gamma**(i)\n",
    "            discounted_rewards[i]=cum_rewards\n",
    "        return cum_rewards\n",
    "    ## now  we optimize policy over trajectories and maintain the dimension of the gradient\n",
    "    ## via the asset command\n",
    "    \n",
    "    \n",
    "    def optimize(self,s,ep):\n",
    "        self.rewards -= np.mean(self.rewards)\n",
    "        self.rewards /= np.std(self.rewards)\n",
    "        for t in range(len(self.states)):\n",
    "            grad_log=self.grad_log(self.states[t])\n",
    "            grad_log=np.array(grad_log)\n",
    "            discounted_rewards=self.discount_rewards()\n",
    "            #if len(self.theta) != len(self.actions):\n",
    "            #self.theta=np.reshape(self.theta,len(self.actions))\n",
    "            #print(self.theta)\n",
    "            self.theta = (self.theta)+ self.alpha *grad_log*discounted_rewards\n",
    "                \n",
    "        # print(self.theta)\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.rewards = [] \n",
    "    def save(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.986151   0.32458668 0.54916104 0.50562218]\n",
      "[[0.98453059 0.34134064 0.55064097 0.48574232]\n",
      " [0.98766153 0.30896892 0.54778147 0.52415385]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-28fa10bc6eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mepi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode:{} with score {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-8b783cad2a58>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, s, ep)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mgrad_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mgrad_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mdiscounted_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-8b783cad2a58>\u001b[0m in \u001b[0;36mgrad_log\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(x[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mgrad_p0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mgrad_p1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 4)"
     ]
    }
   ],
   "source": [
    "import agent\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "state=env.reset()\n",
    "\n",
    "score=0\n",
    "epi=0\n",
    "state_size=4\n",
    "action_size=env.action_space.n\n",
    "pol=PolicyG(state_size, action_size)\n",
    "\n",
    "MAX_EP=1000\n",
    "\n",
    "while epi<MAX_EP:\n",
    "    actione, prob= pol.act(np.array(state))\n",
    "    state,reward,done, info=env.step(actione)\n",
    "    \n",
    "    if done:\n",
    "        reward= -10\n",
    "    score+=reward\n",
    "    pol.memory(state,actione,prob,reward)\n",
    "    \n",
    "    if done:\n",
    "        epi+=1\n",
    "    \n",
    "        pol.optimize(state,epi)\n",
    "    \n",
    "        print('Episode:{} with score {}'.format(epi,score))\n",
    "        score=0\n",
    "        state=env.reset()\n",
    "        if epi% 10 == 0:\n",
    "            pol.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
